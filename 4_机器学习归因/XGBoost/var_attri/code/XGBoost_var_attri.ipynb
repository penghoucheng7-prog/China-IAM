{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f6dd566-d753-4193-9353-9c6a1d4bad15",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b24d5-d0fd-4de3-bb81-3a418ae98dd6",
   "metadata": {},
   "source": [
    "## å¾®è°ƒ+Morris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89eba3-15f3-40c9-90a2-87b77e461f5f",
   "metadata": {},
   "source": [
    "### ä¿®æ”¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aea07d92-0e39-4e5b-bc99-0e0623a55fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, re, psutil, shap, matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from interpret.blackbox import MorrisSensitivity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score,\n",
    "    mean_squared_error, mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import matplotlib\n",
    "\n",
    "# â€”â€” å­—ä½“é…ç½®ï¼šå®‹ä½“ + Times â€”â€”\n",
    "config = {\n",
    "    \"font.family\": 'serif',\n",
    "    \"font.serif\": ['SimSun'],\n",
    "    \"mathtext.fontset\": 'stix',\n",
    "    \"font.size\": 12,\n",
    "    \"axes.unicode_minus\": False\n",
    "}\n",
    "matplotlib.rcParams.update(config)\n",
    "\n",
    "# â€”â€” è·¯å¾„é…ç½® â€”â€”\n",
    "INPUT = r\"C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\data\\var_attri_data_interp_cleaned.csv\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def sanitize(s):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '_', s)\n",
    "\n",
    "def check_memory(gb=1):\n",
    "    avail = psutil.virtual_memory().available / (1024**3)\n",
    "    if avail < gb:\n",
    "        raise MemoryError(f\"å†…å­˜ä¸è¶³: {avail:.2f}â€¯GB\")\n",
    "        \n",
    "def analyze(var, df, output_dir, fine_tune_regions=[\"CHN\",\"R10CHINA+\"]):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    # æ–°å¢ï¼šåŒæ—¶å‰”é™¤æ‰€æœ‰åŒ…å«è‡ªå˜é‡åçš„åˆ—\n",
    "    vars_cols = [\n",
    "        c for c in df.columns\n",
    "        if c not in id_cols and var not in c\n",
    "    ]\n",
    "    data = df[id_cols + vars_cols].dropna()\n",
    "    X, y = data[vars_cols], data[var].values\n",
    "\n",
    "    # â€”â€” æ¨¡å‹ï¼šXGBoost â€”â€”\n",
    "    et = XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.7,\n",
    "        random_state=42,\n",
    "        tree_method='auto',\n",
    "        verbosity=0\n",
    "    )\n",
    "    et.fit(X, y)\n",
    "    mask = data['Region'].isin(fine_tune_regions)\n",
    "    X_sub, y_sub = X[mask], y[mask]\n",
    "    et.set_params(n_estimators=60)\n",
    "    et.fit(X_sub, y_sub)\n",
    "\n",
    "    check_memory()\n",
    "\n",
    "    # â€”â€”â€” æ€§èƒ½è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_sub, y_sub, test_size=0.3, random_state=42)\n",
    "    preds_te = et.predict(Xte)\n",
    "    metrics = {\n",
    "        \"R2_in\": r2_score(ytr, et.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, preds_te),\n",
    "        \"EVS_out\": explained_variance_score(yte, preds_te),\n",
    "        \"MSE_out\": mean_squared_error(yte, preds_te),\n",
    "        \"MAE_out\": mean_absolute_error(yte, preds_te),\n",
    "        \"MedAE_out\": median_absolute_error(yte, preds_te),\n",
    "    }\n",
    "    tag = sanitize(var)\n",
    "    outdir = os.path.join(output_dir, tag)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    pd.DataFrame([metrics]).to_csv(\n",
    "        os.path.join(outdir, f\"{tag}_æ¨¡å‹æŒ‡æ ‡.csv\"),\n",
    "        index=False, encoding='utf-8-sig'\n",
    "    )\n",
    "\n",
    "    # â€”â€”â€” ç‰¹å¾é‡è¦æ€§\n",
    "    imp = et.feature_importances_\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"å˜é‡\": vars_cols,\n",
    "        \"ç›¸å¯¹é‡è¦æ€§(%)\": np.round(imp / imp.sum() * 100, 4)\n",
    "    }).nlargest(10, \"ç›¸å¯¹é‡è¦æ€§(%)\")\n",
    "    imp_df.to_csv(\n",
    "        os.path.join(outdir, f\"{tag}_å˜é‡ç›¸å¯¹é‡è¦æ€§.csv\"),\n",
    "        index=False, encoding='utf-8-sig'\n",
    "    )\n",
    "\n",
    "    # â€”â€”â€” SHAP åˆ†æï¼šæ³¨æ„é‡‡æ ·ç”¨å…¨éƒ¨è®­ç»ƒç‰¹å¾\n",
    "    sample = X_sub.sample(n=min(100, len(X_sub)), random_state=42)\n",
    "    expl = shap.TreeExplainer(et, feature_perturbation=\"auto\")\n",
    "    shap_vals = expl.shap_values(sample, approximate=True, check_additivity=False)\n",
    "\n",
    "    # åªç”» top_feats\n",
    "    top_feats = imp_df[\"å˜é‡\"].tolist()\n",
    "    top_idx = [sample.columns.get_loc(f) for f in top_feats]\n",
    "\n",
    "    # SHAP summary_plot åªä¼  top_feats çš„å€¼å’Œåå­—\n",
    "    shap.summary_plot(\n",
    "        shap_vals[:, top_idx],\n",
    "        sample[top_feats],\n",
    "        feature_names=top_feats,\n",
    "        show=False, max_display=10\n",
    "    )\n",
    "    fig = plt.gcf()\n",
    "    ax = fig.axes[0]; ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "    cbar = fig.axes[-1]; cbar.set_ylabel(\"\"); cbar.set_yticklabels([]); cbar.set_xticklabels([])\n",
    "\n",
    "    # åªå¯¹top_featsè®¡ç®—å‡å€¼/æ ‡å‡†å·®\n",
    "    mean_shap = np.abs(shap_vals[:, top_idx]).mean(axis=0)\n",
    "    ordered_idx = np.argsort(mean_shap)[::-1]\n",
    "    for y_loc, idx in enumerate(ordered_idx):\n",
    "        mv = mean_shap[idx]\n",
    "        ax.hlines(y=y_loc, xmin=mv - 0.01, xmax=mv + 0.01,\n",
    "                  color=\"black\", linewidth=12, zorder=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, f\"{tag}_SHAP.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    pd.DataFrame({\n",
    "        \"å˜é‡\": [top_feats[i] for i in ordered_idx],\n",
    "        \"SHAPå‡å€¼\": np.round(mean_shap[ordered_idx], 6)\n",
    "    }).to_csv(\n",
    "        os.path.join(outdir, f\"{tag}_mean_SHAP.csv\"),\n",
    "        index=False, encoding='utf-8-sig'\n",
    "    )\n",
    "\n",
    "    mean_shap_df = pd.DataFrame({\n",
    "        \"å˜é‡\": [top_feats[i] for i in ordered_idx],\n",
    "        \"å‡å€¼_SHAP\": np.round(mean_shap[ordered_idx], 6),\n",
    "        \"ç‰¹å¾é‡è¦æ€§\": np.round(imp_df[\"ç›¸å¯¹é‡è¦æ€§(%)\"].values[ordered_idx] / 100, 6),\n",
    "        \"SHAP_æ ‡å‡†å·®\": np.round(np.std(shap_vals[:, top_idx][:, ordered_idx], axis=0), 6)\n",
    "    })\n",
    "    mean_shap_df.to_csv(os.path.join(outdir, f\"{tag}_SHAP_vs_Imp.csv\"),\n",
    "                        index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # ğŸ“Š å›¾1. SHAP vs ç‰¹å¾é‡è¦æ€§æŸ±çŠ¶å›¾\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    df_plot = mean_shap_df.sort_values(\"å‡å€¼_SHAP\", ascending=True)\n",
    "    y = np.arange(len(df_plot))\n",
    "    ax.barh(y - 0.2, df_plot[\"å‡å€¼_SHAP\"], height=0.4, label=\"SHAPå‡å€¼\")\n",
    "    ax.barh(y + 0.2, df_plot[\"ç‰¹å¾é‡è¦æ€§\"], height=0.4, label=\"ç‰¹å¾é‡è¦æ€§\")\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(df_plot[\"å˜é‡\"])\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, f\"{tag}_SHAP_vs_Imp_bar.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # ğŸ“ˆ å›¾2. Morris Âµ* vs Ïƒ æ•£ç‚¹å›¾\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sc = ax.scatter(\n",
    "        mean_shap_df[\"å‡å€¼_SHAP\"],\n",
    "        mean_shap_df[\"SHAP_æ ‡å‡†å·®\"],\n",
    "        s=100 * mean_shap_df[\"ç‰¹å¾é‡è¦æ€§\"],\n",
    "        c=mean_shap_df[\"ç‰¹å¾é‡è¦æ€§\"],\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.8,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "    for _, row in mean_shap_df.iterrows():\n",
    "        ax.text(row[\"å‡å€¼_SHAP\"], row[\"SHAP_æ ‡å‡†å·®\"], row[\"å˜é‡\"], fontsize=8)\n",
    "    ax.set_xlabel(\"Morris Âµ* (SHAPå‡å€¼)\")\n",
    "    ax.set_ylabel(\"Ïƒ (SHAPæ ‡å‡†å·®)\")\n",
    "    plt.colorbar(sc, label=\"ç‰¹å¾é‡è¦æ€§\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(outdir, f\"{tag}_Morris_scatter.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # ğŸ“‰ å›¾3. SHAP Dependence Plotï¼ˆå‰ 3 ä¸ªå˜é‡ï¼‰\n",
    "    for i, feat in enumerate([top_feats[i] for i in ordered_idx[:3]]):\n",
    "        interaction_index = 'auto'\n",
    "        if interaction_index == 'auto':\n",
    "            interactions = shap.utils.approximate_interactions(\n",
    "                feat, shap_vals, sample\n",
    "            )\n",
    "            interaction_feat = sample.columns[interactions[0]]\n",
    "        else:\n",
    "            interaction_feat = interaction_index\n",
    "\n",
    "        x = sample[feat].values\n",
    "        y = shap_vals[:, sample.columns.get_loc(feat)]\n",
    "        color = sample[interaction_feat].values if interaction_feat != feat else None\n",
    "\n",
    "        shap.dependence_plot(\n",
    "            sample.columns.get_loc(feat), shap_vals, sample,\n",
    "            feature_names=sample.columns.tolist(),\n",
    "            interaction_index=interaction_index, show=False\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        png_path = os.path.join(outdir, f\"{tag}_SHAP_dependence_{sanitize(feat)}.png\")\n",
    "        plt.savefig(png_path, dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        data = {\n",
    "            feat: x,\n",
    "            f\"SHAP_{feat}\": y,\n",
    "        }\n",
    "        if color is not None:\n",
    "            data[interaction_feat] = color\n",
    "        df = pd.DataFrame(data)\n",
    "        csv_path = os.path.join(outdir, f\"{tag}_SHAP_dependence_{sanitize(feat)}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"[âœ”] {var} ä½¿ç”¨ XGBoost åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼š{outdir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94cd31-b28d-4755-97e8-d419f38c9817",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sobol æ¼æ–—å¼éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28a21661-77f0-4cb5-8735-5e7f9cc3ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor  # âœ… æ›¿æ¢ä¸ºXGBoost\n",
    "\n",
    "def run_sobol_analysis(X, y, var_names, output_dir, tag=\"fei\"):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X[var_names])\n",
    "\n",
    "    problem = {\n",
    "        'num_vars': len(var_names),\n",
    "        'names': var_names,\n",
    "        'bounds': [[0, 1]] * len(var_names)\n",
    "    }\n",
    "\n",
    "    # é‡‡æ · + æ¨¡æ‹Ÿ\n",
    "    param_values = saltelli.sample(problem, 512, calc_second_order=True)\n",
    "    # ==== æ›¿æ¢ä¸ºXGBRegressor ====\n",
    "    model = XGBRegressor(n_estimators=100, max_depth=6, random_state=42, verbosity=0)\n",
    "    model.fit(X_scaled, y)\n",
    "    Y = model.predict(param_values)\n",
    "\n",
    "    # Sobol åˆ†æ\n",
    "    sobol_result = sobol.analyze(problem, Y, calc_second_order=True, print_to_console=False)\n",
    "\n",
    "    # ä¿å­˜ç»“æœ\n",
    "    df_out = pd.DataFrame({\n",
    "        \"å˜é‡\": var_names,\n",
    "        \"ä¸€é˜¶S1\": np.round(sobol_result['S1'], 4),\n",
    "        \"æ€»æ•ˆST\": np.round(sobol_result['ST'], 4),\n",
    "        \"S1_conf\": np.round(sobol_result['S1_conf'], 4),\n",
    "        \"ST_conf\": np.round(sobol_result['ST_conf'], 4)\n",
    "    })\n",
    "\n",
    "    outpath = os.path.join(output_dir, f\"{tag}_sobol.csv\")\n",
    "    df_out.to_csv(outpath, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[âœ”] Sobol æ€»æ•ˆåº”åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š{outpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a544c2e3-9558-4e9a-9e3a-0ae7bd4e715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgroup_shap_analysis_model(var, df, output_dir, fine_tune_regions):\n",
    "    df_china = df[df['Region'].isin(fine_tune_regions)].copy()\n",
    "    model_vals = df_china[\"Model\"].unique()\n",
    "\n",
    "    for model_name in model_vals:\n",
    "        df_sub = df_china[df_china[\"Model\"] == model_name].copy()\n",
    "        if len(df_sub) < 40:\n",
    "            print(f\"[è·³è¿‡] Model={model_name} æ ·æœ¬è¿‡å°‘ï¼ˆ{len(df_sub)} æ¡ï¼‰\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"[â–¶] æ­£åœ¨åˆ†æ Model={model_name}...\")\n",
    "            sub_output = os.path.join(output_dir, f\"China_Model_{sanitize(str(model_name))}\")\n",
    "            # analyzeå‡½æ•°éœ€å…¼å®¹XGBoostï¼Œå‚æ•°é¡ºåºä¿æŒä¸€è‡´\n",
    "            analyze(var, df_sub, sub_output, fine_tune_regions)\n",
    "        except Exception as e:\n",
    "            print(f\"[âŒ] Model={model_name} åˆ†æå¤±è´¥ï¼š{e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a80c9-2cf3-446c-839b-a280dd988ab1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# fei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c82d9d7f-6172-4e64-b57a-1f3f6ded8ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] fei ä½¿ç”¨ XGBoost åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®å¹¶æ ¼å¼è½¬æ¢\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# å•å˜é‡è°ƒç”¨\n",
    "analyze('fei', df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff892c91-65ca-476a-9bdc-d3f801efd6c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å­é›†ç‰¹å¾å†è®­ç»ƒä¸éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19685ffc-d49d-4390-96a6-1d87b1a793b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼ˆè¿½åŠ å¢æ ‘å¾®è°ƒï¼‰ï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒï¼ˆåŸºçº¿æ¨¡å‹40æ£µæ ‘ï¼‰\n",
    "    xgb_reg_base = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg_base.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ ä¸­å›½æ ·æœ¬è¿½åŠ å¾®è°ƒï¼ˆå†åŠ 20æ ‘ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg_ft = xgb.XGBRegressor(\n",
    "            n_estimators=60,  # æ€»å…±60æ£µæ ‘ï¼Œå‰40ä¸ºåŸºçº¿\n",
    "            max_depth=6,\n",
    "            min_child_weight=5,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1,\n",
    "            tree_method='auto'\n",
    "        )\n",
    "        xgb_reg_ft.fit(X_china, y_china, xgb_model=xgb_reg_base.get_booster())\n",
    "        final_model = xgb_reg_ft\n",
    "    else:\n",
    "        final_model = xgb_reg_base\n",
    "\n",
    "    # 3ï¸âƒ£ è®­ç»ƒ/æµ‹è¯•é›†è¯„ä¼°ï¼ˆåœ¨å…¨éƒ¨æ ·æœ¬ä¸Šï¼‰\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, final_model.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, final_model.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, final_model.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, final_model.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, final_model.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, final_model.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# =================== ä¸»ä½“æµç¨‹ ===================\n",
    "linear_feats    = [\"fel\", \"pec\", \"fee\"]\n",
    "nonlinear_feats = [\"seeh\", \"pe\", \"eced\"]\n",
    "shap_top5_feats = [\"seeh\", \"pe\", \"eced\", \"fel\", \"pec\", \"fee\", \"peo\", \"seeg\", \"emcoen\", \"eces\"]\n",
    "\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, linear_feats,    \"çº¿æ€§ä¸»æ•ˆå˜é‡\",    outdir))\n",
    "results.append(evaluate_subset(target_var, df, nonlinear_feats, \"éçº¿æ€§äº¤äº’å˜é‡\", outdir))\n",
    "results.append(evaluate_subset(target_var, df, shap_top5_feats, \"SHAPå‰äº”å˜é‡\",   outdir))\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_å­é›†å»ºæ¨¡æ¯”è¾ƒç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼ˆè¿½åŠ å¢æ ‘å¾®è°ƒï¼‰ï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36f642-87ad-410a-bf9c-6a1771f188b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sobol æ¼æ–—å¼éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "410c67a4-2480-4f64-a585-16eb5dbe3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰å®šç›®æ ‡å˜é‡å’Œç‰¹å¾ç»„åˆï¼ˆä¾‹å¦‚åŸºäº Morris ç­›é€‰ï¼‰\n",
    "target_var = \"fei\"\n",
    "sobol_vars = [\"seeh\", \"pe\", \"eced\"]  # å¯æ ¹æ® Morris Ïƒ é«˜å˜é‡é€‰æ‹©\n",
    "\n",
    "# æ•°æ®å‡†å¤‡\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# ä»…å–ä¸­å›½åŒºåŸŸæ•°æ®\n",
    "fine_tune_regions = [\"CHN\", \"R10CHINA+\"]\n",
    "df_china = df[df['Region'].isin(fine_tune_regions)].dropna(subset=[target_var] + sobol_vars)\n",
    "\n",
    "X = df_china[sobol_vars]\n",
    "y = df_china[target_var].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "267db574-ad9d-4df6-af9c-4d408c4b8314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Local\\Temp\\ipykernel_1800\\633809096.py:17: DeprecationWarning: `salib.sample.saltelli` will be removed in SALib 1.5.1 Please use `salib.sample.sobol`\n",
      "  param_values = saltelli.sample(problem, 512, calc_second_order=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] Sobol æ€»æ•ˆåº”åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei_sobol_sobol.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Roaming\\Python\\Python310\\site-packages\\SALib\\util\\__init__.py:274: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  names = list(pd.unique(groups))\n"
     ]
    }
   ],
   "source": [
    "run_sobol_analysis(X, y, sobol_vars, OUTPUT_DIR, tag=\"fei_sobol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93a8a5-c3ff-43cb-ae47-e9801be7d2f9",
   "metadata": {},
   "source": [
    "## æ¨¡å‹é™ç»´&äº¤äº’å»ºæ¨¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625ea26-7e84-4abe-aead-5bdf4b973563",
   "metadata": {},
   "source": [
    "### åªç”¨eced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e478a73-0b35-46f0-8ffa-34e7406c7c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åªç”¨ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # Step 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Step 2ï¸âƒ£ å¦‚æœä¸­å›½å­æ ·æœ¬å……è¶³ï¼Œè¿½åŠ æ ‘å¾®è°ƒ\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # Step 3ï¸âƒ£ åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === åªç”¨ecedåšå›å½’ ===\n",
    "feature_subset = ['eced']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "# ä½ å¯ä»¥forå¾ªç¯max_depth=4,5,6å¤šæ¬¡ï¼Œè¿™é‡Œä»¥4ä¸ºä¾‹\n",
    "results.append(evaluate_subset(target_var, df, feature_subset, \"åªç”¨eced\", outdir, max_depth=4))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_ecedå•å˜é‡å›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… åªç”¨ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568214bb-22fb-4a04-b04c-a3459d611e96",
   "metadata": {},
   "source": [
    "### ecedè¿›è¡Œä¸‰æ®µå“‘å˜é‡å›å½’+eced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb122e70-a97b-44b5-b942-6fb907bae3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ecedä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # Step 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Step 2ï¸âƒ£ â€œè¿½åŠ æ ‘â€å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆè€Œä¸æ˜¯è¦†ç›–ï¼ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # Step 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === æ„é€ ä¸‰æ®µå“‘å˜é‡ ===\n",
    "df['ec_low'] = (df['eced'] <= 6000).astype(int)\n",
    "df['ec_mid'] = ((df['eced'] > 6000) & (df['eced'] <= 9000)).astype(int)\n",
    "df['ec_high'] = (df['eced'] > 9000).astype(int)\n",
    "\n",
    "# === ä¸‰æ®µå“‘å˜é‡ + åŸå§‹ecedåšå›å½’ ===\n",
    "feature_subset = ['ec_low', 'ec_mid', 'ec_high', 'eced']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(target_var, df, feature_subset, \"ecedä¸‰æ®µå“‘å˜é‡+åŸå§‹\", outdir, max_depth=4)\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_ecedä¸‰æ®µå“‘å˜é‡åŠ åŸå§‹å›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… ecedä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82c873-fc92-4ecd-ac9d-6e607a87a009",
   "metadata": {},
   "source": [
    "### ecedã€peã€seeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0223ce-a659-4bf9-8abf-20c08508fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eced+pe+seehå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # Step 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒï¼ˆn_estimators=40ï¼‰\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Step 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆè¿½åŠ 20æ£µæ ‘ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # Step 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === åªç”¨ecedã€peã€seehåšå›å½’ ===\n",
    "feature_subset = ['eced', 'pe', 'seeh']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, feature_subset, \"eced+pe+seeh\", outdir, max_depth=4))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_eced_pe_seehå›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… eced+pe+seehå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef09ddb-e276-43c7-922d-3018266864ec",
   "metadata": {},
   "source": [
    "### ecedã€peã€seeh+pe Ã— fet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22497eb0-9e19-404b-b75b-c52a34de4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eced+pe+seeh+pe_fetå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # Step 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒï¼ˆå¦‚40æ£µæ ‘ï¼‰\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Step 2ï¸âƒ£ â€œè¿½åŠ æ ‘â€å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆå¦‚è¿½åŠ 20æ£µæ ‘ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # Step 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === æ„é€ äº¤äº’é¡¹ ===\n",
    "df['pe_fet'] = df['pe'] * df['fet']\n",
    "\n",
    "# === ç‰¹å¾é›† ===\n",
    "feature_subset = ['eced', 'pe', 'seeh', 'pe_fet']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, feature_subset, \"eced+pe+seeh+pe_fet\", outdir, max_depth=4))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_eced_pe_seeh_pefetå›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… eced+pe+seeh+pe_fetå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddb62f-e376-43e4-ade7-ca4a45854384",
   "metadata": {},
   "source": [
    "### ecedã€peã€seeh+seehÃ—eced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a70bdf-8397-488d-914f-87e96c5377e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eced+pe+seeh+seeh_ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆä¸è¦†ç›–ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === æ„é€ äº¤äº’é¡¹ ===\n",
    "df['seeh_eced'] = df['seeh'] * df['eced']\n",
    "\n",
    "# === ç‰¹å¾é›† ===\n",
    "feature_subset = ['eced', 'pe', 'seeh', 'seeh_eced']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(target_var, df, feature_subset, \"eced+pe+seeh+seeh_eced\", outdir, max_depth=4)\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_eced_pe_seeh_seehecedå›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… eced+pe+seeh+seeh_ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47128caf-59b5-4bb4-ac52-606060bf36fb",
   "metadata": {},
   "source": [
    "### ecedã€peã€seeh+pe Ã— fet+seehÃ—eced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4999ca84-414a-45c0-b973-85f7c5a8b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… eced+pe+seeh+pe_fet+seeh_ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\fei\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir, max_depth=4):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # Step 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # Step 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆè¿½åŠ 20æ£µæ ‘ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # Step 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === æ„é€ äº¤äº’é¡¹ ===\n",
    "df['pe_fet'] = df['pe'] * df['fet']\n",
    "df['seeh_eced'] = df['seeh'] * df['eced']\n",
    "\n",
    "# === ç‰¹å¾é›† ===\n",
    "feature_subset = ['eced', 'pe', 'seeh', 'pe_fet', 'seeh_eced']\n",
    "target_var = 'fei'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"fei\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, feature_subset, \"eced+pe+seeh+pe_fet+seeh_eced\", outdir, max_depth=4))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"fei_eced_pe_seeh_pefet_seehecedå›å½’ç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… eced+pe+seeh+pe_fet+seeh_ecedå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab43a8-34e9-4559-8861-50ffeb24ad09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# peo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad660bac-3f10-4358-8c49-9e11ef0a0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] peo ä½¿ç”¨ XGBoost åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# å•å˜é‡è°ƒç”¨\n",
    "analyze('peo', df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9c80d-bfb0-4374-a37f-9a3242fb5b1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å­é›†ç‰¹å¾å†è®­ç»ƒä¸éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f6ca64-63b9-4263-8dfa-26b844affadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬åŸºçº¿æ¨¡å‹ï¼ˆ40æ£µæ ‘ï¼‰\n",
    "    xgb_reg_base = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg_base.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ ä¸­å›½å­æ ·æœ¬å¢æ ‘å¾®è°ƒ\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    final_model = xgb_reg_base\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg_finetune = xgb.XGBRegressor(\n",
    "            n_estimators=60,  # å¢åŠ åˆ°60æ£µæ ‘\n",
    "            max_depth=6,\n",
    "            min_child_weight=5,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1,\n",
    "            tree_method='auto'\n",
    "        )\n",
    "        # å…³é”®ï¼šç”¨ xgb_model=åŸºçº¿æ¨¡å‹boosterï¼Œå¢æ ‘è€Œéè¦†ç›–\n",
    "        xgb_reg_finetune.fit(X_china, y_china, xgb_model=xgb_reg_base.get_booster())\n",
    "        final_model = xgb_reg_finetune\n",
    "\n",
    "    # 3ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ/æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, final_model.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, final_model.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, final_model.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, final_model.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, final_model.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, final_model.predict(Xte)),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41c47f6c-fb06-444d-a128-bfad80feb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_feats = [\"seeh\", \"seec\",\"pecwc\"]\n",
    "nonlinear_feats = [\"pe\", \"seeo\",\"fel\"]\n",
    "shap_top5_feats = [\"pe\", \"seeo\", \"fel\", \"fet\", \"seeh\",\"seec\",\"pecwc\",\"seegwoc\",\"pen\",\"ecese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbdc832a-5cf7-40ce-8e42-e18132b79278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, linear_feats, \"çº¿æ€§ä¸»æ•ˆå˜é‡\", outdir))\n",
    "results.append(evaluate_subset(target_var, df, nonlinear_feats, \"éçº¿æ€§äº¤äº’å˜é‡\", outdir))\n",
    "results.append(evaluate_subset(target_var, df, shap_top5_feats, \"SHAPå‰åå˜é‡\", outdir))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"peo_å­é›†å»ºæ¨¡æ¯”è¾ƒç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d84d81-73b6-4e90-941b-be0dbddba184",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sobol æ¼æ–—å¼éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4444a8f7-c795-434d-b7aa-385011efc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰å®šç›®æ ‡å˜é‡å’Œç‰¹å¾ç»„åˆï¼ˆä¾‹å¦‚åŸºäº Morris ç­›é€‰ï¼‰\n",
    "target_var = \"peo\"\n",
    "sobol_vars = [\"pe\", \"seeo\",\"fel\"]  # å¯æ ¹æ® Morris Ïƒ é«˜å˜é‡é€‰æ‹©\n",
    "\n",
    "# æ•°æ®å‡†å¤‡\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# ä»…å–ä¸­å›½åŒºåŸŸæ•°æ®\n",
    "fine_tune_regions = [\"CHN\", \"R10CHINA+\"]\n",
    "df_china = df[df['Region'].isin(fine_tune_regions)].dropna(subset=[target_var] + sobol_vars)\n",
    "\n",
    "X = df_china[sobol_vars]\n",
    "y = df_china[target_var].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "accd4668-4dea-42ea-ba01-fcb6ab359f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Local\\Temp\\ipykernel_1800\\633809096.py:17: DeprecationWarning: `salib.sample.saltelli` will be removed in SALib 1.5.1 Please use `salib.sample.sobol`\n",
      "  param_values = saltelli.sample(problem, 512, calc_second_order=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] Sobol æ€»æ•ˆåº”åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo_sobol_sobol.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Roaming\\Python\\Python310\\site-packages\\SALib\\util\\__init__.py:274: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  names = list(pd.unique(groups))\n"
     ]
    }
   ],
   "source": [
    "run_sobol_analysis(X, y, sobol_vars, OUTPUT_DIR, tag=\"peo_sobol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76fa46-9b33-41a5-b70f-da299cdeb092",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ¨¡å‹é™ç»´&äº¤äº’å»ºæ¨¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b0212-6cd6-4918-aa8e-9c3cd54a1804",
   "metadata": {},
   "source": [
    "### åªç”¨fel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac749c0-f8d9-4999-93b5-6d55f2955dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… felå•å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # æ¯”å¦‚å†è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åªç”¨felåšå›å½’ ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "feature_subset = [\"fel\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"felå•å˜é‡\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_felå•å˜é‡å›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… felå•å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3d8a5-f7d7-4956-a01f-fe153ebaa777",
   "metadata": {},
   "source": [
    "### felè¿›è¡Œä¸‰æ®µå“‘å˜é‡å›å½’+fel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a6e3245-7abb-444b-8a25-853de0a91dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… felä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆè€Œä¸æ˜¯è¦†ç›–ï¼ï¼‰\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === æ„é€ felä¸‰æ®µå“‘å˜é‡ + åŸå§‹fel ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "df['fel_low'] = (df['fel'] <= 15).astype(int)\n",
    "df['fel_mid'] = ((df['fel'] > 15) & (df['fel'] <= 35)).astype(int)\n",
    "df['fel_high'] = (df['fel'] > 35).astype(int)\n",
    "\n",
    "feature_subset = [\"fel_low\", \"fel_mid\", \"fel_high\", \"fel\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"felä¸‰æ®µå“‘å˜é‡+åŸå§‹\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_felä¸‰æ®µå“‘å˜é‡åŠ åŸå§‹å›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… felä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ebdc9-f9e0-4310-b1aa-669c593983f6",
   "metadata": {},
   "source": [
    "### fel+ pe + seeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "423f1c72-b2ed-414d-bf3b-0df7f6d97301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… fel+pe+seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆè€Œä¸æ˜¯è¦†ç›–ï¼ï¼‰\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åªç”¨ fel, pe, seeo åšå›å½’ ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "feature_subset = [\"fel\", \"pe\", \"seeo\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"fel+pe+seeo\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_fel_pe_seeoå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… fel+pe+seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b8996-fd2f-4acb-b9b3-0543f31d8b64",
   "metadata": {},
   "source": [
    "### fel+ pe + seeo+fel*pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4c48711-d8f1-4b49-951f-88830b20d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… fel+pe+seeo+fel*peå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬ï¼ˆä¸æ˜¯é‡æ–°fitï¼ï¼‰\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === æ„é€  fel*pe äº¤äº’é¡¹ ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['fel_pe_interaction'] = df['fel'] * df['pe']\n",
    "\n",
    "feature_subset = [\"fel\", \"pe\", \"seeo\", \"fel_pe_interaction\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"fel+pe+seeo+fel*pe\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_fel_pe_seeo_felpeå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… fel+pe+seeo+fel*peå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e0ea1-f092-419e-ae11-77027f07f3f3",
   "metadata": {},
   "source": [
    "### fel+ pe + seeo+fel*seeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbcb8c3f-71df-45c5-8427-a1b782b631ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… fel+pe+seeo+fel*seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === æ„é€  fel*seeo äº¤äº’é¡¹ ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['fel_seeo_interaction'] = df['fel'] * df['seeo']\n",
    "\n",
    "feature_subset = [\"fel\", \"pe\", \"seeo\", \"fel_seeo_interaction\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"fel+pe+seeo+fel*seeo\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_fel_pe_seeo_felseeoå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… fel+pe+seeo+fel*seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d87504-a590-409b-9572-895088275ad0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### fel+ pe + seeo+fel*pe+fel*seeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7564a41-f2c8-474b-900b-4ab6606e0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… fel+pe+seeo+fel*pe+fel*seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\peo\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df_subset = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_subset[feature_subset], df_subset[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬\n",
    "    mask_china = df_subset['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === æ„é€ äº¤äº’é¡¹ ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['fel_pe_interaction'] = df['fel'] * df['pe']\n",
    "df['fel_seeo_interaction'] = df['fel'] * df['seeo']\n",
    "\n",
    "feature_subset = [\"fel\", \"pe\", \"seeo\", \"fel_pe_interaction\", \"fel_seeo_interaction\"]\n",
    "target_var = 'peo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"peo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var,\n",
    "        df,\n",
    "        feature_subset,\n",
    "        \"fel+pe+seeo+fel*pe+fel*seeo\",\n",
    "        outdir\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"peo_fel_pe_seeo_felpe_felseeoå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… fel+pe+seeo+fel*pe+fel*seeoå˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f78e8e-31b8-4f7c-800d-2f4b59a5a65d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# seeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf58bc99-006e-40d3-8a6a-f832bff2991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] seeo ä½¿ç”¨ XGBoost åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åœ¨ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# å•å˜é‡è°ƒç”¨\n",
    "analyze('seeo', df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64937a5f-bc70-4739-a1c0-a16e3c61b0b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å­é›†ç‰¹å¾å†è®­ç»ƒä¸éªŒè¯Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b81b12e4-5a92-45d4-b875-a70905d536e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "\n",
    "def evaluate_subset(var, df, feature_subset, subset_name, output_dir):\n",
    "    id_cols = ['Model','Scenario','Region','Year', var]\n",
    "    df = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df[feature_subset], df[var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬åŸºçº¿æ¨¡å‹ï¼ˆ40æ£µæ ‘ï¼‰\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=40,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ éè¦†ç›–å¼ä¸­å›½å­æ ·æœ¬å¾®è°ƒï¼ˆå¢æ ‘åˆ°60æ£µæ ‘ï¼‰\n",
    "    mask_china = df['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg_finetune = xgb.XGBRegressor(\n",
    "            n_estimators=60,  # æ¯”åŸºçº¿å¤š20æ£µ\n",
    "            max_depth=6,\n",
    "            min_child_weight=5,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            verbosity=0,\n",
    "            n_jobs=-1,\n",
    "            tree_method='auto'\n",
    "        )\n",
    "        # å…³é”®ç‚¹ï¼šç”¨ xgb_model=åŸºçº¿boosterå®ç°â€œå¢æ ‘â€è€Œä¸æ˜¯è¦†ç›–\n",
    "        xgb_reg_finetune.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "        xgb_reg = xgb_reg_finetune\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f27012e9-f859-43d5-97fe-bcfcf5a828a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_feats = [\"csc\"]\n",
    "nonlinear_feats = [\"peowc\", \"seebwoc\"]\n",
    "shap_top5_feats = [\"peowc\", \"seebwoc\", \"csc\", \"ecese\", \"pen\",\"pe\",\"see\",\"penr\",\"peowoc\",\"seen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac6f3fea-a6b7-4942-8765-02f345bb4ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½æ•°æ®\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "target_var = 'seeo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"seeo\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_subset(target_var, df, linear_feats, \"çº¿æ€§ä¸»æ•ˆå˜é‡\", outdir))\n",
    "results.append(evaluate_subset(target_var, df, nonlinear_feats, \"éçº¿æ€§äº¤äº’å˜é‡\", outdir))\n",
    "results.append(evaluate_subset(target_var, df, shap_top5_feats, \"SHAPå‰åå˜é‡\", outdir))\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(os.path.join(outdir, \"seeo_å­é›†å»ºæ¨¡æ¯”è¾ƒç»“æœ.csv\"), index=False, encoding='utf-8-sig')\n",
    "print(\"âœ… å­é›†å»ºæ¨¡è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ea436-20e4-4b7c-bb99-d9b464ba68ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sobol æ¼æ–—å¼éªŒè¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8b198de-9f95-4e28-9f74-056a64aec8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰å®šç›®æ ‡å˜é‡å’Œç‰¹å¾ç»„åˆï¼ˆä¾‹å¦‚åŸºäº Morris ç­›é€‰ï¼‰\n",
    "target_var = \"seeo\"\n",
    "sobol_vars = [\"peowc\", \"seebwoc\"]  # å¯æ ¹æ® Morris Ïƒ é«˜å˜é‡é€‰æ‹©\n",
    "\n",
    "# æ•°æ®å‡†å¤‡\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# ä»…å–ä¸­å›½åŒºåŸŸæ•°æ®\n",
    "fine_tune_regions = [\"CHN\", \"R10CHINA+\"]\n",
    "df_china = df[df['Region'].isin(fine_tune_regions)].dropna(subset=[target_var] + sobol_vars)\n",
    "\n",
    "X = df_china[sobol_vars]\n",
    "y = df_china[target_var].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355f6be1-6ac6-4e9b-bdad-3efa0a92cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Local\\Temp\\ipykernel_20196\\633809096.py:17: DeprecationWarning: `salib.sample.saltelli` will be removed in SALib 1.5.1 Please use `salib.sample.sobol`\n",
      "  param_values = saltelli.sample(problem, 512, calc_second_order=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ”] Sobol æ€»æ•ˆåº”åˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼šC:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_sobol_sobol.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phc\\AppData\\Roaming\\Python\\Python310\\site-packages\\SALib\\util\\__init__.py:274: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n",
      "  names = list(pd.unique(groups))\n"
     ]
    }
   ],
   "source": [
    "run_sobol_analysis(X, y, sobol_vars, OUTPUT_DIR, tag=\"seeo_sobol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47357c-ce31-40f2-aa76-c36e96e75b92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æ¨¡å‹é™ç»´&äº¤äº’å»ºæ¨¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f689a68-9b9e-4c8d-b450-c09fa58cedc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ä»…ç”¨ peowc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "730768c4-4b91-4e2c-9581-d5599443c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åªç”¨ peowc å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_peowc\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, output_dir, max_depth=5):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # 1ï¸âƒ£ åŸºç¡€æ¨¡å‹è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒåˆ°ä¸­å›½å­æ ·æœ¬\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_china, y_china = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_china, y_china, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†åšè¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    metrics = {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\": r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\": r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\": explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\": mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\": mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === åªç”¨ peowc åšå›å½’ ===\n",
    "feature_subset = ['peowc']\n",
    "target_var = 'seeo'\n",
    "outdir = os.path.join(OUTPUT_DIR, \"seeo_peowc\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"åªç”¨ peowc\",\n",
    "        output_dir=outdir,\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_å•å˜é‡å›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… åªç”¨ peowc å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8f6c1-eca2-4c24-a4c1-a3a4961051df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### å¯¹ peowc åšä¸‰æ®µå“‘å˜é‡+peowc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "880aa10d-3dbf-4f74-9fe9-a8a0ac795062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… peowc ä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_peowc_dummies\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, max_depth=5):\n",
    "    # åªä¿ç•™ id åˆ— + ç‰¹å¾ + ç›®æ ‡ï¼Œå»æ‰ç¼ºå¤±\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨éƒ¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ è¿½åŠ æ ‘å¾®è°ƒä¸­å›½å­æ ·æœ¬\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_ch, y_ch = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # å¢åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_ch, y_ch, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ æ‹†åˆ†æµ‹è¯•é›†åšè¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X_all, y_all, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\":     r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\":    r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\":   explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\":   mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\":   mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === å¯¹ peowc åšä¸‰æ®µå“‘å˜é‡ + åŸå§‹ peowc ===\n",
    "df['peowc_low']  = (df.peowc <= 0.25).astype(int)\n",
    "df['peowc_mid']  = ((df.peowc > 0.25) & (df.peowc <= 1.0)).astype(int)\n",
    "df['peowc_high'] = (df.peowc > 1.0).astype(int)\n",
    "\n",
    "feature_subset = ['peowc_low', 'peowc_mid', 'peowc_high', 'peowc']\n",
    "target_var     = 'seeo'\n",
    "outdir         = os.path.join(OUTPUT_DIR, \"seeo_peowc_dummies\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"peowc_ä¸‰æ®µå“‘å˜é‡+åŸå§‹\",\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_ä¸‰æ®µå“‘å˜é‡åŠ åŸå§‹å›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… peowc ä¸‰æ®µå“‘å˜é‡+åŸå§‹å˜é‡å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c6597-0ac0-40f1-9be3-03ac12ad9dd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### peowc+seebwoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca3cc4aa-6547-4e9a-b5c2-5d2c6d24a516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… peowc + seebwoc å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_peowc_seebwoc\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, max_depth=5):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ ä¸­å›½å­é›†è¿½åŠ æ ‘å¾®è°ƒ\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_ch, y_ch = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_ch, y_ch, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X_all, y_all, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\":     r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\":    r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\":   explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\":   mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\":   mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# === ç‰¹å¾ï¼špeowc + seebwoc ===\n",
    "feature_subset = ['peowc', 'seebwoc']\n",
    "target_var     = 'seeo'\n",
    "outdir         = os.path.join(OUTPUT_DIR, \"seeo_peowc_seebwoc\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"peowc + seebwoc\",\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_seebwocå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… peowc + seebwoc å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053eda4-2715-40a6-b00e-cd28ed13623e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### peowc+seebwoc+seebwoc*cscf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "204cd9a2-03df-49d2-800d-47786b8fbc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… peowc + seebwoc + seebwoc*cscf äº¤äº’å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_peowc_seebwoc_seebwocxcscf\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, max_depth=5):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬åŸºç¡€æ¨¡å‹\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ ä¸­å›½å­é›†å¾®è°ƒï¼šè¿½åŠ æ ‘\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_ch, y_ch = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_ch, y_ch, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # 3ï¸âƒ£ è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X_all, y_all, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\":     r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\":    r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\":   explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\":   mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\":   mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# æ„é€ äº¤äº’ç‰¹å¾ seebwoc Ã— cscf\n",
    "df['seebwoc_x_cscf'] = df['seebwoc'] * df['cscf']\n",
    "\n",
    "# === ç”¨ peowcã€seebwocã€seebwoc_x_cscf åšå›å½’ ===\n",
    "feature_subset = ['peowc', 'seebwoc', 'seebwoc_x_cscf']\n",
    "target_var     = 'seeo'\n",
    "outdir         = os.path.join(OUTPUT_DIR, \"seeo_peowc_seebwoc_seebwocxcscf\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"peowc + seebwoc + seebwoc*cscf\",\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_seebwoc_seebwocxcscfå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… peowc + seebwoc + seebwoc*cscf äº¤äº’å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559d0d7-76bc-4418-b5c1-f652c2247860",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### peowc+seebwoc+peowc*csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5583bf6d-03b8-4145-9602-6899faa798c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… peowc + seebwoc + peowc*csc å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_peowc_seebwoc_peowcxcsc\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, max_depth=5):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # 1ï¸âƒ£ å…¨æ ·æœ¬è®­ç»ƒ\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # 2ï¸âƒ£ ä¸­å›½å­é›†â€œè¿½åŠ æ ‘â€å¾®è°ƒ\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_ch, y_ch = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20  # è¿½åŠ 20æ£µæ ‘\n",
    "        xgb_reg.fit(X_ch, y_ch, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # è®­ç»ƒ/æµ‹è¯•æ‹†åˆ†ä¸è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        X_all, y_all, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\":     r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\":    r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\":   explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\":   mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\":   mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# æ„é€ äº¤äº’ç‰¹å¾ peowc Ã— csc\n",
    "df['peowc_x_csc'] = df['peowc'] * df['csc']\n",
    "\n",
    "# === ç”¨ peowc, seebwoc, peowc_x_csc åšå›å½’ ===\n",
    "feature_subset = ['peowc', 'seebwoc', 'peowc_x_csc']\n",
    "target_var     = 'seeo'\n",
    "outdir         = os.path.join(OUTPUT_DIR, \"seeo_peowc_seebwoc_peowcxcsc\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"peowc + seebwoc + peowc*csc\",\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_seebwoc_peowcxcscå›å½’ç»“æœ.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… peowc + seebwoc + peowc*csc å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a712b-09f4-4060-a5d8-bb1aa0e790da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### peowc+seebwoc+peowc*csc+seebwoc*cscf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a504e0a-3f83-478b-b6e0-c3316561727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… peowc + seebwoc + peowc*csc + seebwoc*cscf å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š C:\\Users\\phc\\Desktop\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ\\ä¸­å›½æ¨¡å‹æ¯”è¾ƒ2\\4_æœºå™¨å­¦ä¹ å½’å› \\XGBoost\\var_attri\\results\\seeo_full_interactions\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, explained_variance_score, mean_squared_error,\n",
    "    mean_absolute_error, median_absolute_error\n",
    ")\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def evaluate_subset(target_var, df, feature_subset, subset_name, max_depth=5):\n",
    "    id_cols = ['Model', 'Scenario', 'Region', 'Year', target_var]\n",
    "    df_sub = df[id_cols + feature_subset].dropna()\n",
    "    X_all, y_all = df_sub[feature_subset], df_sub[target_var].values\n",
    "\n",
    "    # å…¨éƒ¨æ ·æœ¬è®­ç»ƒ100æ£µæ ‘\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=max_depth,\n",
    "        min_child_weight=5,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method='auto'\n",
    "    )\n",
    "    xgb_reg.fit(X_all, y_all)\n",
    "\n",
    "    # å¢é‡å¾®è°ƒï¼šä¸­å›½åŒºåŸŸï¼Œå†åŠ 20æ£µæ ‘\n",
    "    mask_china = df_sub['Region'].isin([\"CHN\", \"R10CHINA+\"])\n",
    "    if mask_china.sum() > 30:\n",
    "        X_ch, y_ch = X_all[mask_china], y_all[mask_china]\n",
    "        xgb_reg.n_estimators += 20\n",
    "        xgb_reg.fit(X_ch, y_ch, xgb_model=xgb_reg.get_booster())\n",
    "\n",
    "    # æ‹†åˆ†è®­ç»ƒ/æµ‹è¯•å¹¶è¯„ä¼°\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=0.3, random_state=42)\n",
    "    return {\n",
    "        \"å­é›†åç§°\": subset_name,\n",
    "        \"ç‰¹å¾æ•°é‡\": len(feature_subset),\n",
    "        \"R2_in\":     r2_score(ytr, xgb_reg.predict(Xtr)),\n",
    "        \"R2_out\":    r2_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"EVS_out\":   explained_variance_score(yte, xgb_reg.predict(Xte)),\n",
    "        \"MSE_out\":   mean_squared_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MAE_out\":   mean_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "        \"MedAE_out\": median_absolute_error(yte, xgb_reg.predict(Xte)),\n",
    "    }\n",
    "\n",
    "# === åŠ è½½æ•°æ® ===\n",
    "df = pd.read_csv(INPUT, encoding='utf-8-sig')\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# æ„é€ äº¤äº’ç‰¹å¾ peowcÃ—csc å’Œ seebwocÃ—cscf\n",
    "df['peowc_x_csc']    = df['peowc']   * df['csc']\n",
    "df['seebwoc_x_cscf'] = df['seebwoc'] * df['cscf']\n",
    "\n",
    "# === ç”¨ peowc, seebwoc, peowc_x_csc, seebwoc_x_cscf åšå›å½’ ===\n",
    "feature_subset = ['peowc', 'seebwoc', 'peowc_x_csc', 'seebwoc_x_cscf']\n",
    "target_var     = 'seeo'\n",
    "outdir         = os.path.join(OUTPUT_DIR, \"seeo_full_interactions\")\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "results.append(\n",
    "    evaluate_subset(\n",
    "        target_var=target_var,\n",
    "        df=df,\n",
    "        feature_subset=feature_subset,\n",
    "        subset_name=\"peowc + seebwoc + peowc*csc + seebwoc*cscf\",\n",
    "        max_depth=5\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_csv(\n",
    "    os.path.join(outdir, \"seeo_peowc_seebwoc_peowcxcsc_seebwocxcscf_results.csv\"),\n",
    "    index=False,\n",
    "    encoding='utf-8-sig'\n",
    ")\n",
    "print(\"âœ… peowc + seebwoc + peowc*csc + seebwoc*cscf å›å½’è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š\", outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
